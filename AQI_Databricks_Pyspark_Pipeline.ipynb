{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8bea31f-e832-47f3-8f84-a6d310969534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"s3://demobucketdatabricks/raw_data/Indian_Climate_Dataset_2024_2025.csv\") # Reading the data from raw s3 folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991617ad-ca30-41fb-86dd-474663b5e29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, to_date\n",
    "\n",
    "df_raw_chunks = df_raw \\\n",
    "    .withColumn(\"Date\", to_date(\"Date\")) \\\n",
    "    .withColumn(\"year\", year(\"Date\")) \\\n",
    "    .withColumn(\"month\", month(\"Date\"))  # Adding year and month columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d962e0fe-156f-46da-b343-562a4fad7e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw_chunks.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .csv(\"s3://demobucketdatabricks/raw_data/raw_partition_data/\") # Writing the partitioned raw data to s3 with folder structure \n",
    "\n",
    "    # This is are bronze layer data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a449bda-2952-42aa-bee5-b0eadfe2bb47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"s3://demobucketdatabricks/raw_data/raw_partition_data/*/*\")  # Reading the partitioned raw data from s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4d31fb-8648-4f77-9cf6-18b46ea12cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ece93ce-e947-431d-b1f4-044594d2f9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_silver = df_bronze \\\n",
    "    .withColumn(\"Temperature_Max_C\", col(\"Temperature_Max (°C)\").cast(\"double\")) \\\n",
    "    .withColumn(\"Temperature_Min_C\", col(\"Temperature_Min (°C)\").cast(\"double\")) \\\n",
    "    .withColumn(\"Temperature_Avg_C\", col(\"Temperature_Avg (°C)\").cast(\"double\")) \\\n",
    "    .withColumn(\"Humidity_Percent\", col(\"Humidity (%)\").cast(\"double\")) \\\n",
    "    .withColumn(\"Rainfall_mm\", col(\"Rainfall (mm)\").cast(\"double\")) \\\n",
    "    .withColumn(\"Wind_Speed_kmh\", col(\"Wind_Speed (km/h)\").cast(\"double\")) \\\n",
    "    .withColumn(\"Pressure_hPa\", col(\"Pressure (hPa)\").cast(\"double\")) \\\n",
    "    .withColumn(\"Cloud_Cover_Percent\", col(\"Cloud_Cover (%)\").cast(\"double\")) \\\n",
    "    .withColumn(\"AQI\", col(\"AQI\").cast(\"int\"))\n",
    "\n",
    "    # Here we are standardizing column names and converting/casting the data types to appropriate types and this will be our silver layer data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c559a00d-e82b-4457-9230-a0934d014210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.show(10) # After casting and standardizing our data we have duplicate columns in our data, which we will drop from our dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15861595-05b9-4c30-8898-c9d9b8d2a031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver = df_silver.drop(\n",
    "    \"Temperature_Max (°C)\",\n",
    "    \"Temperature_Min (°C)\",\n",
    "    \"Temperature_Avg (°C)\",\n",
    "    \"Humidity (%)\",\n",
    "    \"Rainfall (mm)\",\n",
    "    \"Wind_Speed (km/h)\",\n",
    "    \"Pressure (hPa)\",\n",
    "    \"Cloud_Cover (%)\"\n",
    ") # Dropping the duplicate columns from our dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e948aea7-21a1-415c-afb0-ff33ff9d1a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67fbe532-47a0-484d-b106-d67cb0bc6346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Remove invalid temperatures\n",
    "df_silver_clean = df_silver.filter(\n",
    "    (col(\"Temperature_Max_C\").isNotNull()) &\n",
    "    (col(\"Temperature_Min_C\").isNotNull()) &\n",
    "    (col(\"Temperature_Max_C\") >= col(\"Temperature_Min_C\"))\n",
    ")\n",
    "\n",
    "# Remove invalid AQI\n",
    "df_silver_clean = df_silver_clean.filter(col(\"AQI\") >= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58c2549-041b-45a7-b9b7-d4a3b2241e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_clean.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52619283-a196-497c-8eeb-66ac455c05d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, to_date\n",
    "df_silver_clean = df_silver_clean.withColumn(\"Date\", to_date(\"Date\"))\n",
    "df_silver_clean = df_silver_clean \\\n",
    "    .withColumn(\"year\", year(\"Date\")) \\\n",
    "    .withColumn(\"month\", month(\"Date\"))\n",
    "df_silver_clean.printSchema() # Again same process as above, adding year and month columns to save the data in silver layer in a structured format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1e3c39-d6cd-46e2-90fa-8262e9e79d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_clean.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3://demobucketdatabricks/curated_data/curated_partition_data/\") # Writing the silver layer data to s3 with folder structure in parquet formatb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b67093d-e410-4aff-8a06-2ecf6e1f5009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_clean = spark.read.parquet(\"s3://demobucketdatabricks/curated_data/curated_partition_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6db894a6-d1e1-443f-ab87-89d7f466c546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_silver_clean.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98619ed-042d-49ad-b5c3-4a0905e688b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reconcilation proecss between bronze and silver layer to check if any data is missing while data transformation\n",
    "\n",
    "bronze_count = df_bronze.count()\n",
    "silver_count = df_silver_clean.count()\n",
    "\n",
    "print(f\"Bronze Count: {bronze_count}\")\n",
    "print(f\"Silver Count: {silver_count}\")\n",
    "print(f\"Difference: {bronze_count - silver_count}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b1cbe2-1d54-48f8-b74b-3e507baf38b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gold layer data preparation for final data reporting\n",
    "\n",
    "from pyspark.sql.functions import avg, sum, max, min\n",
    "\n",
    "df_gold_city_month = df_silver_clean.groupBy(\"City\", \"State\", \"year\", \"month\") \\\n",
    "    .agg(\n",
    "        avg(\"Temperature_Avg_C\").alias(\"avg_temp_c\"),\n",
    "        max(\"Temperature_Max_C\").alias(\"max_temp_c\"),\n",
    "        min(\"Temperature_Min_C\").alias(\"min_temp_c\"),\n",
    "        sum(\"Rainfall_mm\").alias(\"total_rainfall_mm\"),\n",
    "        avg(\"AQI\").alias(\"avg_aqi\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e5cc2be-a9f2-43c2-ad49-072d49c57591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_city_month.count()\n",
    "\n",
    "df_gold_city_month.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad7b041-2fca-42b1-aa47-fed2564beb19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "window = Window.partitionBy(\"year\", \"month\").orderBy(col(\"avg_aqi\").desc())\n",
    "\n",
    "df_gold_final = df_gold_city_month \\\n",
    "    .withColumn(\"pollution_rank\", rank().over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e02678-e3db-4a1c-94fc-e091a21271c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(\"s3://demobucketdatabricks/final_data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b95393-ab35-42d4-9ac4-6a88e36383fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_final.show(5)\n",
    "df_gold_final.printSchema()\n",
    "df_gold_final.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a0d92d0-f1ba-454c-93b6-dd0a094c1254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"s3://demobucketdatabricks/final_data/CSV/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13db1441-d762-42ce-8de7-ba77f743cfe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold_final.count()\n",
    "df_gold_final.select(\"year\", \"month\").distinct().show()\n",
    "df_gold_final.filter(col(\"avg_aqi\").isNull()).count()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AQI_Databricks_Pyspark_Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
